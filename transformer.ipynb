{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/Pinkwjp/sentiment-analysis-with-transformer-on-IMDB)"
      ],
      "metadata": {
        "id": "ceIibQUhdXwV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99FtGlbaICie"
      },
      "outputs": [],
      "source": [
        "# pipenv install --python 3.10\n",
        "# pipenv shell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjox6IF3JxLd"
      },
      "source": [
        "**run this cell when running on colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoGQMZjmICil",
        "outputId": "dbb33d48-cffb-4c62-c1cf-960077a25a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 1s (52.8 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.25.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.10.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.10.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "# create folder\n",
        "from pathlib import Path\n",
        "\n",
        "trained_model_folder = Path(\"./trained_models/\")\n",
        "\n",
        "if not trained_model_folder.exists():\n",
        "    trained_model_folder.mkdir()\n",
        "\n",
        "# install tree\n",
        "!apt-get install tree\n",
        "\n",
        "# install libraries\n",
        "%pip install -q --upgrade keras-nlp  # install keras-nlp before keras\n",
        "%pip install keras-tuner\n",
        "%pip install -q --upgrade keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10VfiCnaICin"
      },
      "source": [
        "**Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x_pxn9cCICio"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import keras_nlp\n",
        "from keras_nlp import layers as nlp_layers\n",
        "import keras_tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlAN0hTMT64Q"
      },
      "source": [
        "**Download the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiLAG9lSICip",
        "outputId": "fb2d2594-ed29-479f-984b-a9fe5378290d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "\u001b[1m84125825/84125825\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "keras.utils.get_file(\n",
        "    origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
        "    cache_dir=\"./\",\n",
        "    extract=True\n",
        ")\n",
        "\n",
        "imdb_dir = Path(\"./datasets/aclImdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Q39A3GICiq",
        "outputId": "31f6bb20-8887-4430-98e7-44697d899845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34mdatasets/aclImdb/\u001b[0m\n",
            "├── \u001b[01;34mtest\u001b[0m\n",
            "│   ├── \u001b[01;34mneg\u001b[0m\n",
            "│   └── \u001b[01;34mpos\u001b[0m\n",
            "└── \u001b[01;34mtrain\u001b[0m\n",
            "    ├── \u001b[01;34mneg\u001b[0m\n",
            "    ├── \u001b[01;34mpos\u001b[0m\n",
            "    └── \u001b[01;34munsup\u001b[0m\n",
            "\n",
            "7 directories\n"
          ]
        }
      ],
      "source": [
        "!tree -d datasets/aclImdb/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kOSUbWqT64V"
      },
      "source": [
        "remove unsupervised training data, we don't need that here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JvJEiFXST64V"
      },
      "outputs": [],
      "source": [
        "!rm -r datasets/aclImdb/train/unsup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNfLsbGjICir",
        "outputId": "006be1c1-df7d-4845-c660-70f180800634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34mdatasets/aclImdb/\u001b[0m\n",
            "├── \u001b[01;34mtest\u001b[0m\n",
            "│   ├── \u001b[01;34mneg\u001b[0m\n",
            "│   └── \u001b[01;34mpos\u001b[0m\n",
            "└── \u001b[01;34mtrain\u001b[0m\n",
            "    ├── \u001b[01;34mneg\u001b[0m\n",
            "    └── \u001b[01;34mpos\u001b[0m\n",
            "\n",
            "6 directories\n"
          ]
        }
      ],
      "source": [
        "!tree -d datasets/aclImdb/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM81Pp54T64W"
      },
      "source": [
        "quick look at one review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atmjYAMKT64W",
        "outputId": "c0059693-d380-448a-e89a-7bdc0448eb18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What was with all the Turkish actors? No offense but I thought it was all for nothing for all these actors. The film had no script to test any actors acting skill or ability. It demanded next to nothing I bought this film to see Michael Madsen. He is one of my favorite actors but this film was another failure for him. The script was so bad. Their was just nothing to sink your teeth into and all the characters were two dimensional. Madsen tried to act like a hard ass but the script and direction didn't even allow him to do enough with his character to make it more interesting or 3 dimensional.<br /><br />Even the sound effects of the gunfight at the beginning of the film sounded like the noise of paint ball guns when they are fired in a skirmish. It was really weird and they didn't sound like real guns. A video game had better sound effects than this film. There was also a really annoying bloke at the beginning of the film who was a member of the robbery gang. He had this American whining voice like a girl shouting lines like \"Lets get the F#$k out of here\" and What are we going to do man\". He sounded like a girl. As a positive It was funny to watch and it made me laugh too. For a few seconds. Whoo Hoo ! Dumb Film. Poor Madsen. He will bounce back..."
          ]
        }
      ],
      "source": [
        "!cat datasets/aclImdb/train/neg/21_4.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z66wMQ-dT64X"
      },
      "source": [
        "prepare validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nrLfJ140T64X"
      },
      "outputs": [],
      "source": [
        "import os, shutil, random\n",
        "\n",
        "validation_dir = imdb_dir / \"validation\"\n",
        "validation_dir.mkdir()\n",
        "train_dir = imdb_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    (validation_dir / category).mkdir()\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1234).shuffle(files)  # use seed to ensure same dataset through different runs\n",
        "    num_validation_samples = int(0.2 * len(files))\n",
        "    validation_files = files[-num_validation_samples:]\n",
        "    for file in validation_files:\n",
        "        shutil.move(train_dir / category / file,\n",
        "                    validation_dir / category / file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6OVyfH7ICit",
        "outputId": "52d636ac-bb26-48c3-df36-8f3f8e231679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01;34mdatasets/aclImdb/\u001b[0m\n",
            "├── \u001b[01;34mtest\u001b[0m\n",
            "│   ├── \u001b[01;34mneg\u001b[0m\n",
            "│   └── \u001b[01;34mpos\u001b[0m\n",
            "├── \u001b[01;34mtrain\u001b[0m\n",
            "│   ├── \u001b[01;34mneg\u001b[0m\n",
            "│   └── \u001b[01;34mpos\u001b[0m\n",
            "└── \u001b[01;34mvalidation\u001b[0m\n",
            "    ├── \u001b[01;34mneg\u001b[0m\n",
            "    └── \u001b[01;34mpos\u001b[0m\n",
            "\n",
            "9 directories\n"
          ]
        }
      ],
      "source": [
        "!tree -d datasets/aclImdb/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYV5zS6XT64X",
        "outputId": "ed56c5bd-3007-4dd8-f241-1512eb23e091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# 0 for negative, 1 for positive\n",
        "train_dataset = keras.utils.text_dataset_from_directory(\n",
        "    \"datasets/aclImdb/train\", batch_size=batch_size)\n",
        "validation_dataset = keras.utils.text_dataset_from_directory(\n",
        "    \"datasets/aclImdb/validation\", batch_size=batch_size)\n",
        "test_dataset = keras.utils.text_dataset_from_directory(\n",
        "    \"datasets/aclImdb/test\", batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWF_yM8oT64Y"
      },
      "source": [
        "take a look at the batch data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8mSyCqYT64Y",
        "outputId": "c083488d-fafa-4ddc-a409-cd2b53959747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape:  (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'Alone in the Dark is Uwe Boll\\'s kick in the nuts to Hollywood after House of the Dead\\'s punch in the face.<br /><br />If anything it proves just how much of a master manipulator Boll is. After forcing Artisan out of business over the flop that was House of the Dead, one can only assume the normally credible Lion\\'s Gate Films only released AITD under contractual obligation after acquiring Artisan\\'s assets. Because AITD is an even bigger example of complete lack of coherent film-making ability, plot exposition and just plain stealing poorly from other movies because it was supposed to look cool instead of because it fitted within the movie\\'s framework.<br /><br />But then that\\'s the point, isn\\'t it. Boll isn\\'t trying to make a coherent film because he isn\\'t trying to direct Alone in the Dark. He\\'s just trying to manipulate Hollywood.<br /><br />Alone in the Dark, like House of the Dead, Dungeon Siege, Far Cry, Bloodrayne and the other 3 or 4 projects that are \"announced\" or in \"pre-production\".<br /><br />These aren\\'t movies to be directed, but investment portfolios. Every single one of them rushed into production under the pretence that the tax law Boll and his investors are exploiting may be closed within the next 2 to 3 years. The more bomb projects he can release within that time-frame, the more money he and his investors can gain. Why bother making a good movie when a bad movie\\'s making you a mint anyway? The result is movies like the awfulness of Alone in the Dark.<br /><br />Alone in the Dark, like all his other movies are just a cynical exploitation of Hollywood\\'s current trend for lazy film-making.<br /><br />And to those who support Boll by calling him misunderstood or the next Ed Wood, congratulations, by making a cult figure out of the man, you\\'re just making it easier for him to get investors but giving him notoriety.<br /><br />For more information, read here: http://www.cinemablend.com/feature.php?id=209 http://www.cinemablend.com/forum/showthread.php?s=&threadid=21699 As an aside, just don\\'t ask me how he\\'s getting his cast-lists together. Unless the actors are in on the investment-scam somehow, that mystery has still to be uncovered.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_dataset:\n",
        "    print(\"inputs.shape: \", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdUCKZXfICiu"
      },
      "source": [
        "train a text vectorization layer with unlabeled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qmcDi88cICiu"
      },
      "outputs": [],
      "source": [
        "text_only_train_dataset = train_dataset.map(lambda x, y: x)  # do not need labels to train the text vectorization layer\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20_000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length)\n",
        "\n",
        "text_vectorization.adapt(text_only_train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YBOnN7eT64Z"
      },
      "source": [
        "prepare integer sequence datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TIGkJSgQT64Z"
      },
      "outputs": [],
      "source": [
        "int_train_dataset = train_dataset.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_validation_dataset = validation_dataset.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_dataset = test_dataset.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kK9QrWBT64a"
      },
      "source": [
        "use a TransformerEncoder-based model for text classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "uLi1x4vDT64a",
        "outputId": "03a23d01-2aef-4bdf-abc9-18c7749c130d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │         \u001b[38;5;34m659,200\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │           \u001b[38;5;34m6,464\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m660\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m21\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">659,200</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">660</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m666,345\u001b[0m (2.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">666,345</span> (2.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m666,345\u001b[0m (2.54 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">666,345</span> (2.54 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "vocabulary_size = 20_000\n",
        "sequence_length = 600\n",
        "embed_dimension = 32\n",
        "\n",
        "num_heads = 2\n",
        "dense_layer_dimension = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "x = nlp_layers.TokenAndPositionEmbedding(vocabulary_size=vocabulary_size,\n",
        "                                         sequence_length=sequence_length,\n",
        "                                         embedding_dim=embed_dimension\n",
        "                                         )(inputs)\n",
        "\n",
        "x = nlp_layers.TransformerEncoder(intermediate_dim=dense_layer_dimension,\n",
        "                                  num_heads=num_heads\n",
        "                                  )(x)\n",
        "\n",
        "# x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)  # 0.5\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model_1 = keras.Model(inputs, outputs)\n",
        "model_1.compile(optimizer=\"adam\", # rmsprop\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E740hFZbT64a"
      },
      "source": [
        "train the transformer encoder model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaRnc5OxT64a",
        "outputId": "6b8c59f7-ea34-4e28-ac64-a87f8741097d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 26ms/step - accuracy: 0.5546 - loss: 0.6682 - val_accuracy: 0.8760 - val_loss: 0.2943\n",
            "Epoch 2/2\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 17ms/step - accuracy: 0.8942 - loss: 0.2676 - val_accuracy: 0.8900 - val_loss: 0.2910\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78e9f64df6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"trained_models/basic_transformer_encoder.keras\",\n",
        "        save_best_only=True)]\n",
        "\n",
        "model_1.fit(int_train_dataset,\n",
        "            validation_data=int_validation_dataset,\n",
        "            epochs=2,\n",
        "            callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59kyt5UNICiw"
      },
      "source": [
        "evaluate the transformer encoder model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl1Sb-WHICiw",
        "outputId": "85ad633d-9066-43d0-abbc-1c5be16cbbae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8814 - loss: 0.3103\n",
            "Test accuracy: 0.880\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model(\n",
        "    \"trained_models/basic_transformer_encoder.keras\")\n",
        "\n",
        "print(f\"Test accuracy: {model.evaluate(int_test_dataset)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tune a transfromer encoder"
      ],
      "metadata": {
        "id": "AvGd-GPmiAHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_tuner import HyperParameters\n",
        "\n",
        "def build_model(hp: HyperParameters):\n",
        "    inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "    vocabulary_size = hp.Int(\"vocabulary_size\", min_value=20_000, max_value=40_000, step=10_000)\n",
        "    sequence_length = hp.Int(\"sequence_length\", min_value=300, max_value=600, step=100)\n",
        "    embed_dimension = hp.Choice(\"embed_dimension\", [32, 64])\n",
        "\n",
        "    x = nlp_layers.TokenAndPositionEmbedding(\n",
        "            vocabulary_size=vocabulary_size,\n",
        "            sequence_length=sequence_length,\n",
        "            embedding_dim=embed_dimension\n",
        "            )(inputs)\n",
        "\n",
        "    num_heads = hp.Choice(\"num_heads\", [2, 3, 4])\n",
        "    intermediate_dim = hp.Choice(\"intermediate_dim\", [16, 32, 64])\n",
        "\n",
        "    x = nlp_layers.TransformerEncoder(intermediate_dim=intermediate_dim,\n",
        "                                    num_heads=num_heads\n",
        "                                    )(x)\n",
        "\n",
        "    # x = layers.GlobalMaxPooling1D()(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dropout(0.1)(x)  # 0.5\n",
        "    x = layers.Dense(20, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model_1 = keras.Model(inputs, outputs)\n",
        "    model_1.compile(optimizer=\"adam\", # rmsprop\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "    model_1.summary()\n"
      ],
      "metadata": {
        "id": "WsTasOaVh_yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***The following is for comaprison***"
      ],
      "metadata": {
        "id": "hUj_GSazm_UM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "try closely follow a example on Keras website:  \n",
        "https://keras.io/examples/nlp/text_classification_with_transformer/"
      ],
      "metadata": {
        "id": "E0aKhpjcj34N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0-xFulskHb4",
        "outputId": "bfaa6c97-bf90-41b3-aa13-eb3aed691cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = nlp_layers.TokenAndPositionEmbedding(vocab_size, maxlen, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "encoder = nlp_layers.TransformerEncoder(ff_dim, num_heads)\n",
        "x = encoder(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "pNKdbAFPkZvt",
        "outputId": "fe1571f2-98ef-4580-d9f3-9e56c644e919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m646,400\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m6,464\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │             \u001b[38;5;34m660\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │              \u001b[38;5;34m42\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">646,400</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,464</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">660</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m653,566\u001b[0m (2.49 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">653,566</span> (2.49 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m653,566\u001b[0m (2.49 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">653,566</span> (2.49 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: run the example model with my dataset to see if the size of training set makes the difference"
      ],
      "metadata": {
        "id": "rzHDFrJHnjbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        \"trained_models/new_transformer_encoder.keras\",\n",
        "        save_best_only=True)]\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=2,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    callbacks=callbacks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUKqAQ4XlEr1",
        "outputId": "140e48f9-5427-4960-cb6a-4b0432b243be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 24ms/step - accuracy: 0.7158 - loss: 0.5174 - val_accuracy: 0.8810 - val_loss: 0.2805\n",
            "Epoch 2/2\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.9245 - loss: 0.1998 - val_accuracy: 0.8712 - val_loss: 0.3056\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}